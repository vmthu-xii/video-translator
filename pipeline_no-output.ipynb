{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12040729,"sourceType":"datasetVersion","datasetId":7476116}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Download & Install","metadata":{}},{"cell_type":"code","source":"!pip install -q ffmpeg-python TTS","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import librosa\nimport ffmpeg\nimport torch\nimport logging\n# from torch.serialization import safe_globals\nimport builtins\nfrom TTS.api import TTS\nfrom transformers import AutoProcessor, AutoModelForSpeechSeq2Seq, AutoTokenizer, AutoModelForSeq2SeqLM, MarianMTModel, MarianTokenizer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!git clone https://github.com/bytedance/LatentSync.git\n%cd LatentSync \n!pip install -q -r requirements.txt\n!source setup_env.sh","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"stt_id_model = \"openai/whisper-large-v3-turbo\"\nstt_processor = AutoProcessor.from_pretrained(stt_id_model, language=\"vi\", task=\"transcribe\")\nstt_model = AutoModelForSpeechSeq2Seq.from_pretrained(stt_id_model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"t2t_id_model = \"vinai/vinai-translate-vi2en-v2\"\nt2t_tokenizer = AutoTokenizer.from_pretrained(t2t_id_model, src_lang=\"vi_VN\")\nt2t_model = AutoModelForSeq2SeqLM.from_pretrained(t2t_id_model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fix UnpicklingError when loading xtts_v2\norig_torch_load = torch.load\n\ndef torch_wrapper(*args, **kwargs):\n    logging.warning(\"[comfyui-unsafe-torch] I have unsafely patched `torch.load`.  The `weights_only` option of `torch.load` is forcibly disabled.\")\n    kwargs['weights_only'] = False\n\n    return orig_torch_load(*args, **kwargs)\n\ntorch.load = torch_wrapper\n\nNODE_CLASS_MAPPINGS = {}\n__all__ = ['NODE_CLASS_MAPPINGS']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"builtins.input = lambda prompt=\"\": \"y\" # auto enter 'y'\ntts_model = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\", gpu=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Pipeline","metadata":{}},{"cell_type":"markdown","source":"## 1. Extract audio from audio - ffmpeg","metadata":{}},{"cell_type":"code","source":"def getAudio(src, dst):\n    (\n        ffmpeg\n        .input(src)\n        .output(dst, ac=1, ar='16000')  # mono, 16kHz\n        .overwrite_output()\n        .run()\n    )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Automatic Speech Recognition - [whisper-large-v3-turbo](https://github.com/openai/whisper)","metadata":{}},{"cell_type":"code","source":"def speech2Text(src, model, processor):\n    speech, sr = librosa.load(src, sr=16000)\n    input_features = processor(speech, sampling_rate=16000, return_tensors=\"pt\").input_features\n    with torch.no_grad():\n        predicted_ids = model.generate(input_features)\n        vi_text = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n    \n    return vi_text","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Text to Text - [vinai-translate-vi2en ](https://github.com/VinAIResearch/VinAI_Translate)","metadata":{}},{"cell_type":"code","source":"def translate(vi_text, model, tokenizer):\n    input_ids = tokenizer(vi_text, padding=True, return_tensors=\"pt\")\n    output_ids = model.generate(\n        **input_ids,\n        decoder_start_token_id=tokenizer.lang_code_to_id[\"en_XX\"],\n        num_return_sequences=1,\n        num_beams=5,\n        early_stopping=True\n    )\n    en_text = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]\n    \n    return en_text","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Text to Speech - [XTTS-v2](https://github.com/coqui-ai/TTS)","metadata":{}},{"cell_type":"code","source":"def text2Speech(en_text, model, save_path, speaker_wav):\n    model.tts_to_file(text=en_text, file_path=save_path, speaker_wav=speaker_wav, language=\"en\") # default sample rate: 24000 \n    return","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Audio to Video - [LatentSync](https://github.com/bytedance/LatentSync/tree/main)","metadata":{}},{"cell_type":"code","source":"import subprocess\n\ndef audio2video(src_video, src_audio, dst_video, checkpoint=\"checkpoints/latentsync_unet.pt\", steps=10, scale=1.0):\n    cmd = [\n        \"python\", \"-m\", \"scripts.inference\",\n        \"--unet_config_path\", \"configs/unet/stage2.yaml\",\n        \"--inference_ckpt_path\", checkpoint,\n        \"--video_path\", src_video,\n        \"--audio_path\", src_audio,\n        \"--video_out_path\", dst_video,\n        \"--inference_steps\", str(steps),\n        \"--guidance_scale\", str(scale),\n    ]\n    subprocess.run(cmd, check=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"def inference(vi_video, vi_audio, en_audio, en_video, stt_model=stt_model, stt_processor=stt_processor, t2t_model=t2t_model, t2t_tokenizer=t2t_tokenizer):\n    import time\n    start = time.time()\n    \n    getAudio(vi_video, vi_audio)\n    \n    vi_text = speech2Text(vi_audio, stt_model, stt_processor)\n    print('Extract Vietnamese text:', vi_text)\n    \n    en_text = translate(vi_text, t2t_model, t2t_tokenizer)\n    print('Translate to English text:', en_text)\n    \n    text2Speech(en_text, tts_model, en_audio, vi_audio)\n    print('Convert English text to speech oke')\n    \n    audio2video(vi_video, en_audio, en_video)\n    print('Generate new video oke')\n\n    end = time.time()\n    \n    print(f\"Inference time: {end - start:.2f}s\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"VI_VIDEO = \"/kaggle/input/vid-translator/video.mp4\"\nVI_AUDIO = \"/kaggle/working/vi_audio-1.wav\"\nEN_AUDIO = \"/kaggle/working/en_audio-1.wav\"\nEN_VIDEO = \"/kaggle/working/en_video-1.mp4\"\n\ninference(VI_VIDEO, VI_AUDIO, EN_AUDIO, EN_AUDIO)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import Audio\nAudio(EN_AUDIO)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import moviepy.editor\nmoviepy.editor.ipython_display(EN_VIDEO)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"VI_VIDEO = \"/kaggle/input/vid-translator/video-3.mp4\"\nVI_AUDIO = \"/kaggle/working/vi_audio-3.wav\"\nEN_AUDIO = \"/kaggle/working/en_audio-3.wav\"\nEN_VIDEO = \"/kaggle/working/en_video-3.mp4\"\n\ninference(VI_VIDEO, VI_AUDIO, EN_AUDIO, EN_AUDIO)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import Audio\nAudio(EN_AUDIO)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import moviepy.editor\nmoviepy.editor.ipython_display(EN_VIDEO)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}